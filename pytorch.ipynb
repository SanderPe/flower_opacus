{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/10 [00:00<?, ?epoch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06cd3a755d9e4c45b1be7dd461dfd430"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from numpy import vstack\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from opacus import PrivacyEngine\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "LR = 1e-3\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset():\n",
    "    # load the dataset\n",
    "    def __init__(self):\n",
    "        # Load dataset\n",
    "        data = pd.read_csv('../scripts/lightpath_data.csv')\n",
    "        data = data.loc[:, ~data.columns.isin(['sample','conn_id','src_id','dst_id'])]\n",
    "\n",
    "\n",
    "        target = pd.read_csv('../scripts/lightpath_target.csv')\n",
    "        target = target['class']\n",
    "        data['target'] = target\n",
    "        data = data.sample(100000)\n",
    "        target = data['target']\n",
    "        data = data.loc[:, data.columns!='target']\n",
    "        target = np.array(target).reshape(len(target),1)\n",
    "        data = data.values\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler = scaler.fit(data)\n",
    "        data = scaler.transform(data)\n",
    "\n",
    "\n",
    "        self.X = data\n",
    "        self.y = target\n",
    "        self.X = self.X.astype('float32')\n",
    "        self.y = self.y.astype('float32')\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "# prepare the dataset\n",
    "def prepare_data():\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset()\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_dl = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "train_dl, test_dl = prepare_data()\n",
    "\n",
    "\n",
    "# model definition\n",
    "class neural_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(32, 64)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(64, 256)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(256, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = neural_network()\n",
    "# ----------------------------------------------------\n",
    "criterion = BCELoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# train the model\n",
    "def train_model(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    losses = []\n",
    "    top1_acc = []\n",
    "\n",
    "\n",
    "    for i, (inputs, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute output\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "        labels = target.detach().cpu().numpy()\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc = accuracy(preds, labels)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        top1_acc.append(acc)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 200 == 0:\n",
    "\n",
    "            print(\n",
    "                f\"\\tTrain Epoch: {epoch} \\t\"\n",
    "                f\"Loss: {np.mean(losses):.6f} \"\n",
    "                f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Epoch\", unit=\"epoch\"):\n",
    "    train_model(model, train_dl, optimizer, epoch + 1)\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    prec = precision_score(actuals, predictions)\n",
    "    recc = recall_score(actuals, predictions)\n",
    "    f1 = f1_score(actuals, predictions)\n",
    "\n",
    "    return acc, prec, recc, f1\n",
    "\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.916\n",
      "Recall: 0.927\n",
      "Precision: 0.965\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc, prec, recc, f1 = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "print('Recall: %.3f' % prec)\n",
    "print('Precision: %.3f' % recc)\n",
    "print('Precision: %.3f' % f1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sander\\desktop\\env_pyth\\lib\\site-packages\\opacus\\privacy_engine.py:131: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
      "c:\\users\\sander\\desktop\\env_pyth\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:333: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  f\"Optimal order is the {extreme} alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sigma=0.3520774841308594 and C=1.2\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/10 [00:00<?, ?epoch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc076614c8c34b08bbaecc436925dd76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sander\\desktop\\env_pyth\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 23.13 0.62 0.25\n",
      "1 28.1 0.61 0.22\n",
      "2 31.52 0.61 0.25\n",
      "3 34.94 0.62 0.24\n",
      "4 37.86 0.62 0.26\n",
      "5 40.29 0.63 0.25\n",
      "6 42.72 0.63 0.24\n",
      "7 45.14 0.63 0.25\n",
      "8 47.57 0.63 0.24\n",
      "9 50.0 0.64 0.27\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from numpy import vstack\n",
    "\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "\n",
    "from opacus import PrivacyEngine\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "MAX_GRAD_NORM = 1.2\n",
    "EPSILON = 50.0\n",
    "DELTA = 1e-6\n",
    "EPOCHS = 10\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self):\n",
    "        # Load dataset\n",
    "        data = pd.read_csv('../scripts/lightpath_data.csv')\n",
    "        data = data.loc[:, ~data.columns.isin(['sample','conn_id','src_id','dst_id'])]\n",
    "\n",
    "\n",
    "        target = pd.read_csv('../scripts/lightpath_target.csv')\n",
    "        target = target['class']\n",
    "        data['target'] = target\n",
    "        data = data.sample(100000)\n",
    "        target = data['target']\n",
    "        data = data.loc[:, data.columns!='target']\n",
    "        target = np.array(target).reshape(len(target),1)\n",
    "        data = data.values\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler = scaler.fit(data)\n",
    "        data = scaler.transform(data)\n",
    "\n",
    "        self.X = data\n",
    "        self.y = target\n",
    "        self.X = self.X.astype('float32')\n",
    "        self.y = self.y.astype('float32')\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "# prepare the dataset\n",
    "def prepare_data():\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset()\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_dl = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "train_dl, test_dl = prepare_data()\n",
    "\n",
    "\n",
    "# model definition\n",
    "class neural_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(32, 64)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(64, 256)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(256, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = neural_network()\n",
    "# ----------------------------------------------------\n",
    "criterion = BCELoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# enter PrivacyEngine\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_dl,\n",
    "    epochs=EPOCHS,\n",
    "    target_epsilon=EPSILON,\n",
    "    target_delta=DELTA,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")\n",
    "# ----------------------------------------------------\n",
    "print(f\"Using sigma={optimizer.noise_multiplier} and C={MAX_GRAD_NORM}\")\n",
    "# train the model\n",
    "def train_model(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    losses = []\n",
    "    top1_acc = []\n",
    "\n",
    "    for i, (inputs, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute output\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "        labels = target.detach().cpu().numpy()\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc = accuracy(preds, labels)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        top1_acc.append(acc)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epsilon = privacy_engine.get_epsilon(DELTA)\n",
    "        if (i+1) % 200 == 0:\n",
    "            # epsilon = privacy_engine.get_epsilon(DELTA)\n",
    "            print(\n",
    "                f\"\\tTrain Epoch: {epoch} \\t\"\n",
    "                f\"Loss: {np.mean(losses):.6f} \"\n",
    "                f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
    "                f\"(ε = {epsilon:.2f}, δ = {DELTA})\"\n",
    "            )\n",
    "    return round(epsilon,2), round(np.mean(losses),2), round(acc,2)\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Epoch\", unit=\"epoch\"):\n",
    "    epsilon, loss, acc = train_model(model, train_loader, optimizer, epoch + 1)\n",
    "    print(epoch , epsilon, loss, acc)\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    prec = precision_score(actuals, predictions)\n",
    "    recc = recall_score(actuals, predictions)\n",
    "    f1 = f1_score(actuals, predictions)\n",
    "\n",
    "    return acc, prec, recc, f1\n",
    "\n",
    "acc = evaluate_model(test_dl, model)\n",
    "\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sander\\desktop\\env_pyth\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.758\n",
      "Recall: 0.758\n",
      "Precision: 1.000\n",
      "Precision: 0.862\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc, prec, recc, f1 = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "print('Recall: %.3f' % prec)\n",
    "print('Precision: %.3f' % recc)\n",
    "print('Precision: %.3f' % f1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}